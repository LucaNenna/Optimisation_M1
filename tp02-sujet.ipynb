{"cells": [{"cell_type": "markdown", "source": ["***\n", "# **M1MAO -- M1 MA 2020/2021 -- Universit\u00e9 Paris-Saclay**\n", "***"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "code", "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import matplotlib.tri as tri\n", "import numpy as np\n", "import matplotlib.tri as tri"], "outputs": [], "execution_count": 13, "metadata": {}}, {"cell_type": "markdown", "source": ["# TP2 :  Optimisation sous contraintes\n", "##  Le probl\u00e8me d'obstacle\n", "\n", "$$\n", "\\newcommand{\\sca}[2]{\\langle #1\\mid #2 \\rangle}\n", "\\newcommand{\\nr}[1]{\\left\\|#1\\right\\|}\n", "\\newcommand{\\LL}{\\mathrm{L}} \n", "\\newcommand{\\Rsp}{\\mathbb{R}}\n", "\\newcommand{\\Psp}{\\mathbb{P}}\n", "\\newcommand{\\Class}{\\mathcal{C}}\n", "\\newcommand{\\id}{\\mathrm{id}}\n", "$$\n", "Soit $\\Omega$ un ouvert de $\\Rsp^d$ (avec $d=1,2$) et $\\Psi\\in H^1_0(\\Omega) \\cap \\Class^\\infty(\\Omega)$. On s'int\u00e9resse \u00e0 la r\u00e9solution du probl\u00e8me d'obstacle:\n", "$$\n", "\\min_{v\\in C} \\int_\\Omega \\nr{\\nabla v}^2\n", "$$\n", "o\u00f9 $C = \\{ u\\in H^1_0(\\Omega) \\mid u \\geq \\Psi\\}$.\n", "On discr\u00e9tise ce probl\u00e8me par la m\u00e9thode des \u00e9l\u00e9ments finis en dimension d. \n", "On part d'une triangulation $T_h$ de l'ouvert $\\Omega$, et on note:\n", "* $V_h$ l'espace des \u00e9l\u00e9ments finis $\\Psp_1$ sur $T_h$, de sorte que $V_h\\subseteq V := H^1(\\Omega)$;\n", "* $N_h :=\\dim(V_h)$;\n", "* $x_1,\\dots,x_{N_h}$ les sommets de la triangulation et $\\phi_1,\\dots,\\phi_{N_h}$ les fonctions chapeau correspondantes. On note $J_h$ l'ensemble des indices des sommets du bord.\n", "* $V_{0h} = \\{\u00a0v\\in V_h\\mid \\forall k\\in J_h, v(x_k) = 0 \\}$\n", "* $C_h = \\{ v\\in V_{0h}\\mid \\forall 1\\leq i\\leq N_h,~~ v(x_i) \\geq \\Psi(x_i) \\}$\n", "\n", "Le probl\u00e8me discret est alors donn\u00e9 par \n", "$$\n", "\\min_{v\\in C_h} \\int_\\Omega \\nr{\\nabla v}^2.\n", "$$\n", "\n", "**Q0)** On note $W = (v_1,\\dots,v_{N_h}) \\in \\Rsp^{N_h}$ les valeurs d'une fonction $v\\in V_h$ sur les sommets $x_1,\\dots,x_{N_h}$ (i.e. $v = \\sum_{1\\leq i\\leq N_h} v_i \\phi_i$ o\u00f9 $\\phi_i$ est la base de $V_h$ form\u00e9e des fonctions chapeau). Montrer que le probl\u00e8me peut \u00eatre mis sous la forme\n", "\n", "$$ \\min_{W\\in D_h} J(W):=\\sca{W}{A W} $$\n", "$$D_h = \\{ W\\in \\Rsp^{N_h} \\mid \\forall 1\\leq i\\leq N_h, W_i \\geq \\Psi(x_i) \\hbox{ et } \\forall k\\in J_h,~ W_k = 0\\},$$\n", "\n", "o\u00f9 $A_{ij} = \\int_\\Omega \\sca{\\nabla \\phi_i}{\\nabla \\phi_j}$. Montrer que la matrice $A$ est sym\u00e9trique et d\u00e9finie positive.\n", "\n", "\n", "\n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "markdown", "source": ["### Algorithme du gradient projet\u00e9 \n", "\n", "$$ \\newcommand{\\id}{\\mathrm{id}}\n", "\\newcommand{\\D}{\\mathrm{D}}\n", "$$\n", "L'objectif de cette premiere partie est d'introduire et de demontrer la convergence de l'algorithme du gradient projet\u00e9.\n", "\n", "<div class=\"alert alert-block alert-danger\">\n", "<b>Algorithme du gradient projete</b>\n", "Soit $J \\in \\Class^1(\\Rsp^n)$ et $K\\subseteq \\Rsp^n$ un ensemble convexe ferm\u00e9 non vide. L'algorithme du gradient projet\u00e9 \u00e0 pas constat $\\tau>0$ est donn\u00e9 par :\n", "\n", "\\begin{equation}  \n", "\\label{GP}\n", "x^{(k+1)}=p_K(x^{(x)}-\\tau\\nabla J(x^{(k)})), \n", "\\tag{GP}\n", "\\end{equation}\n", "o\u00f9 $p_K(x)$ est la projection d'un point $x\\in\\Rsp^n$ sur $K$.\n", "\n", "</div>\n", "\n", "**Q1)** [**Constante de Lipschitz de $\\id - \\tau \\nabla J$** ] On suppose  que $J\\in \\Class^2(\\Rsp^n)$ et qu'il\n", "    existe deux constantes $L\\geq 0, \\alpha>0$ telles que\n", "    $$ \\forall x,y\\in \\Rsp^n,\\quad \\nr{\\nabla J(x) - \\nabla J(y)} \\leq L\\nr{x - y} $$\n", "    $$ \\forall x,v\\in \\Rsp^n,\\quad \\sca{\\D^2 J(x) v}{v} \\geq \\alpha\n", "    \\nr{v}^2, $$ la deuxi\u00e8me condition impliquant que $\\D^2\n", "    J\\geq\\alpha \\mathrm{Id}_n$ au sens des matrices sym\u00e9triques.\n", "- (i) D\u00e9montrer que si $\\tau \\in (0,2\\alpha/L^2)$, alors l'application $F_\\tau: x\\in \\Rsp^n\\mapsto x - \\tau \\nabla J(x)$ est contractante.   \n", "- (ii) En d\u00e9duire que pour tout $x^{(0)}\\in\\Rsp^n$, la suite d\u00e9finie par $x^{(k+1)} = F_\\tau(x^{(k)})$ converge vers l'unique minimiseur de $J$ sur $\\Rsp^n$\n", "\n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "markdown", "source": ["**Q2)** Dans cette question, on suppose que $J(x) = \\frac{1}{2}\\sca{x}{Ax}$, o\u00f9 $A$ est sym\u00e9trique et d\u00e9finie positive de valeurs propres $0<\\lambda_1\\leq\\cdots\\leq\\lambda_n$.\n", "- (a) Montrer que si $\\tau \\in (0,2/\\lambda_n)$ alors les valeurs propres de la matrice $\\mathrm{Id}_n - \\tau A$ sont dans $(-1,1)$.  \n", "- (b) En d\u00e9duire que pour tout $x^{(0)}\\in\\Rsp^n$, la suite d\u00e9finie par $x^{(k+1)} = x^{(k)} - \\tau \\nabla J(x^{(k)})$ converge vers l'unique minimiseur de $J$ sur $\\Rsp^n$.\n", "\n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "markdown", "source": ["Soit maintenant $J \\in\\Class^1(\\Rsp^n)$ une fonction convexe et soit $K$ un convexe ferm\u00e9 de $\\Rsp^n$. \n", "\n", "<div class=\"alert alert-block alert-danger\">\n", "<b>Projection sur un convexe</b>\n", "  \n", "On note $p=p_K(x)$ la projection d'un point $x\\in \\Rsp^n$ sur $K$, qui par th\u00e9or\u00e8me est l'unique point $p\\in K$ v\u00e9rifiant $\\forall y\\in K,\\quad \\sca{x - p}{p-y} \\geq  0.$\n", "\n", "</div>"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "markdown", "source": ["**Q3)** Montrer que $x \\in \\arg\\min_{K} J$ si et seulement si $\\forall\n", "    y \\in K, \\sca{-\\nabla J(x)}{x-y} \\geq 0$. \n", "    \n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "markdown", "source": ["**Q4)** D\u00e9montrer que l'application $p_K$ est $1$-lipschitzienne.\n", "\n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "markdown", "source": ["**Q5)**  En utilisant la caract\u00e9risation pr\u00e9c\u00e9dente, d\u00e9montrer\n", "    l'\u00e9quivalence entre:\n", "    \n", "- (i) $x$ minimise $J$ sur $K$  ;\n", "\n", "- (ii) $\\exists \\tau>0$ tel que $p_K(x - \\tau\\nabla J(x)) = x$ ;\n", "\n", "- (iii) $\\forall \\tau>0$ tel que $p_K(x - \\tau\\nabla J(x)) = x$ ;\n", "\n", "\n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "markdown", "source": ["**Q6)**  On se place sous les hypoth\u00e8ses de la question **Q1**. Montrer que pour tout $\\tau \\in (0,2\\alpha/L^2),$\n", "  et  tout $x^{(0)}\\in \\Rsp^n$, la suite $x^{(k+1)} := p_K(x^{(k)} - \\tau\\nabla J(x^{(K)}))$ converge vers le minimiseur de $J$ sur $K$, qui est unique.\n", "  \n", "**Q7)** On se place sous les hypoth\u00e8ses de la question **Q2**. Montrer que pour tout $\\tau \\in (0,2/\\lambda_n),$ et tout $x^{(0)}\\in \\Rsp^n$, la suite $x^{(k+1)}:= p_K(x^{(k)} - \\tau\\nabla J(x^{(k)}))$ converge vers un point $x^*$ qui est l'unique minimiseur de $J$ sur $K$.\n", "\n", "**Q8)**  Dans cette question , on suppose $K = \\{V \\in \\Rsp^n \\mid \\forall i, V_i \\geq \\Psi_i\\}$ o\u00f9 $\\Psi\\in\\Rsp^n$ est donn\u00e9e. Montrer que si $\\bar{x} \\in \\arg\\min_K J$ o\u00f9 $J\\in\\Class^1(\\Rsp^n)$, alors $\\lambda := -\\nabla J(\\bar x)$ v\u00e9rifie\n", "\\begin{equation} \n", "\\label{kkt} \n", "\\begin{cases}\n", " \\lambda_i = 0 &\\hbox{ si } \\bar{x}_i > \\Psi_i \\\\ \\lambda_i \\leq 0\n", "&\\hbox{ sinon }\n", "\\end{cases}\n", "\\tag{kkt}\n", "\\end{equation}\n", "\n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "markdown", "source": ["**Q9)** Dans cette question, on suppose que $J\\in\\Class^1(\\Rsp^n)$ est convexe. D\u00e9montrer que si $\\bar{x}\\in K$ et si $\\lambda := -\\nabla J(\\bar x)\\in \\Rsp^n$ v\u00e9rifie l'\u00e9quation \\eqref{kkt}, alors $\\bar{x}$ minimise $J$ sur $K$.\n", "\n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "markdown", "source": ["**Q10)** En d\u00e9duire (on suppose $J$ convexe) que \n", "  $$\\bar x \\in\\arg\\min_K J\\iff \\exists \\lambda\\in\\Rsp^n\\;t.q.\n", "  \\begin{cases}\n", "  &\\bar x \\in K,\\\\\n", "  &-\\nabla J(\\bar x)=\\lambda \\\\\n", "  & \\lambda_i =0\\;\\text{si}\\;\\bar x_i> \\Psi_i\\\\\n", "  & \\lambda_i <0 \\;\\text{si}\\;\\bar x_i= \\Psi_i\\\n", "\\end{cases}$$\n", "\n", "**Q11)** D\u00e9montrer que la projection de $x\\in \\Rsp^n$ sur $K$ est donn\u00e9e par\n", "    $$p_K(x) = (\\max(x_1,\\Psi_1),\\cdots, \\max(x_n,\\Psi_n)).$$"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "markdown", "source": ["**Q12)** Soit $W\\in \\Rsp^{N_h}$. V\u00e9rifier que la projection orthogonale de $W$ sur $D_h$ est donn\u00e9e par\n", "\n", "$$ \\Pi_{D_h}(W) = \\begin{cases} \\max(W_i,\\Psi(x_i)) & \\hbox{ si } i\\not\\in J_h \\\\\n", "0 & \\hbox{ sinon } \n", "\\end{cases}. $$\n", " \u00c9crire une fonction `projDh(W,Psi)` retournant la projection de $W$ sur $D_h$ \n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "code", "source": ["# <completer>\n"], "outputs": [], "execution_count": 14, "metadata": {}}, {"cell_type": "markdown", "source": ["**Q13)** \u00c9crire explicitement l'algorithme de gradient projet\u00e9 (\u00e0 pas constant) dans le cas o\u00f9 $J(W) = W^T A W$ et $K =  D_h$. Expliciter la condition n\u00e9cessaire sur le pas de descente en fonction des valeurs propres de la matrice $A$.\n", "\n", "**Q14)** Impl\u00e9menter cet algorithme dans le cas  $\\Omega = (0,1)$, en utilisant $\\Psi(x)=\\max(1.5-20(x-0.4)^2, 0)$, $N_h=30$. On arretera les it\u00e9rations lorsque $\\nr{u^{(k+1)}-u^{(k)}}\\leq\\epsilon$ avec $\\epsilon=10^{-8}$; stocker le vecteur $G=(\\nr{x^{(k+1)} - x^{(k)}})$ et l'afficher en echelle logaritmique. Utiliser $\\tau=0.015$. Tracer sur la m\u00eame figure la solution approch\u00e9e et la fonction $\\Psi$. Tracer aussi la solution toutes les $10$ iterations.\n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "code", "source": ["# <completer>\n"], "outputs": [], "execution_count": 15, "metadata": {}}, {"cell_type": "markdown", "source": ["**Q15)**  Afficher $2/\\lambda_n$, o\u00f9 $\\lambda_n$ est la plus grande valeure propre de $A$ et reprendre le test de la question precedente en utilisant. le taux optimal $\\tau^\\star=\\frac{\\lambda_1}{\\lambda_n^2}$. Tracer sur la m\u00eame figure la solution approch\u00e9e et la fonction $\\Psi$. Afficher aussi le vecteur $G$ et commenter.\n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "code", "source": ["# <completer>\n"], "outputs": [], "execution_count": 16, "metadata": {}}, {"cell_type": "markdown", "source": ["**Q16)** V\u00e9rifier numeriquement que la solution approch\u00e9e $U$ satisfait les conditions suivantes\n", "\n", "$$ \\begin{cases}\n", "U_i = 0 & \\hbox{ si } i \\in J_h \\hbox{ (bord) } \\\\\n", "- A U_i < 0 & \\hbox{ si } U_i = \\Psi_i \\\\\n", "- A U_i = 0 & \\hbox{ si } U_i > \\Psi_i \n", "\\end{cases} $$"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "code", "source": ["# <completer>\n"], "outputs": [], "execution_count": 17, "metadata": {}}, {"cell_type": "markdown", "source": ["### Le cas d=2\n", "\n"], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "code", "source": ["\n", "\n", "\n", "from mpl_toolkits.mplot3d import Axes3D\n", "\n", "\n", "#triangulation du carr\u00e9 [-1,1]^2\n", "def triangulation_carre(n):\n", "    x,y = np.meshgrid(np.linspace(-1.,1.,n),\n", "                      np.linspace(-1.,1.,n))\n", "    x = x.reshape(n*n,1)\n", "    y = y.reshape(n*n,1)\n", "    X = np.hstack((x,y))\n", "    T = tri.Triangulation(x.flatten(), y.flatten()).triangles\n", "    return X,T\n", "\n", "# afficher la triangulation dont les sommets sont [X[:,0],X[:,1],Z] et les triangles T\n", "def afficher_triangulation_3d(X,T,Z):\n", "    fig = plt.figure(figsize=plt.figaspect(0.5))\n", "    ax = fig.add_subplot(1, 2, 1, projection='3d',)\n", "    ax.plot_trisurf(X[:,0],X[:,1],Z,triangles=T, cmap=plt.cm.Spectral, shade=True)\n", "    plt.show()\n", "\n", "#Matrices de masse et de rigidit\u00e9 sur chaque element de la triangulation\n", "def M_elem(S1,S2,S3):\n", "    x1 = S1[0]\n", "    y1 = S1[1]\n", "    x2 = S2[0] \n", "    y2 = S2[1]\n", "    x3 = S3[0]\n", "    y3 = S3[1]\n", "    D = ((x2-x1)*(y3-y1) - (y2-y1)*(x3-x1))\n", "    M=(1.*np.abs(D)/24)*np.ones([3,3])\n", "    M[range(3),range(3)]=1.*np.abs(D)/12\n", "    return M\n", "\n", "def K_elem(S1,S2,S3):\n", "    x1 = S1[0]\n", "    y1 = S1[1]\n", "    x2 = S2[0] \n", "    y2 = S2[1]\n", "    x3 = S3[0]\n", "    y3 = S3[1]\n", "    norm = np.zeros([3, 2])\n", "    norm[0, :] = np.array([y2-y3, x3-x2])\n", "    norm[1, :] = np.array([y3-y1, x1-x3])\n", "    norm[2, :] = np.array([y1-y2, x2-x1])\n", "    D = ((x2-x1)*(y3-y1) - (y2-y1)*(x3-x1))\n", "    K = np.zeros([3,3])\n", "    for i in range(3):\n", "        for j in range(3):\n", "            K[i,j] = np.dot(norm[i,:],norm[j,:])\n", "    return (1./(2*abs(D)))*K\n", "\n", "\n", "# Assemblage matrices de masse et de rigidit\u00e9\n", "def masse_et_rigidite(X,T):\n", "    NSom = X.shape[0]\n", "    NTri = T.shape[0]\n", "    K = np.zeros([NSom,NSom])\n", "    M = np.zeros([NSom,NSom])\n", "    for N in range(0,NTri):\n", "        S1=X[T[N,0],:]\n", "        S2=X[T[N,1],:]\n", "        S3=X[T[N,2],:]\n", "        Kel=K_elem(S1, S2, S3)\n", "        Mel=M_elem(S1, S2, S3)\n", "        for i in range(0,3): \n", "            I = T[N,i]\n", "            for j in range(0,3): \n", "                J = T[N,j]\n", "                M[I,J] = M[I,J] + Mel[i,j]\n", "                K[I,J] = K[I,J] + Kel[i,j]\n", "    return M,K\n"], "outputs": [], "execution_count": 18, "metadata": {}}, {"cell_type": "markdown", "source": ["**Q1)** Implementer  l'algorithme du gradient projet\u00e9 dans le cas du carr\u00e9  $\\Omega = (-1,1)^2$, en utilisant\n", "$ \\Psi(x,y) = \\kappa(2x^2+2y^2) $\n", "o\u00f9 $$\\kappa(t) = \\begin{cases}\\exp(-1/(1-t^2))\\;&\\text{si}\\;t<1\\\\\n", "                              0\\;&\\text{sinon.}\\end{cases}$$\n", "\n", "On pourra utiliser `np.where` pour construire la liste $J$ des indices des poins du bord. On pourra visualiser le graphe d'une fonction en 3D en utilisant la fonction `afficher_triangulation`.\n", "Afficher toutes les $100$ iterations les points o\u00f9 la contrainte est **active**, $u^{(k)}=\\Psi$. Utiliser $\\tau=0.2$. "], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "code", "source": ["# <completer>\n"], "outputs": [], "execution_count": 19, "metadata": {}}, {"cell_type": "markdown", "source": ["**Q2)** V\u00e9rifier numeriquement que la solution approch\u00e9e $U$ satisfait les conditions de la question **Q16**."], "metadata": {"deletable": false, "editable": false}}, {"cell_type": "code", "source": ["# <completer>\n"], "outputs": [], "execution_count": 20, "metadata": {}}, {"cell_type": "code", "source": [], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": [], "outputs": [], "execution_count": null, "metadata": {}}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "celltoolbar": "None", "language_info": {"pygments_lexer": "ipython3", "version": "3.5.5", "file_extension": ".py", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python"}}, "nbformat": 4, "nbformat_minor": 1}