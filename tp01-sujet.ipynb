{"nbformat_minor": 2, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "celltoolbar": "None", "language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.5.5"}}, "cells": [{"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["***\n", "# **APP. OPT. -- M1 MA 2020/2021 -- Universit\u00e9 Paris-Saclay**\n", "***\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "source": ["import numpy as np\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "def verifier_gradient(f,g,x0):\n", "    N = len(x0)\n", "    gg = np.zeros(N)\n", "    for i in range(N):\n", "        eps = 1e-4\n", "        e = np.zeros(N)\n", "        e[i] = eps\n", "        gg[i] = (f(x0+e) - f(x0-e))/(2*eps)\n", "    print('erreur numerique dans le calcul du gradient: %g (doit etre petit)' % np.linalg.norm(g(x0)-gg))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["$$\\newcommand{\\nr}[1]{\\|#1\\|}\n", "\\newcommand{\\Rsp}{\\mathbb{R}}$$\n", "$\\newcommand{\\sca}[2]{\\langle#1|#2\\rangle}$\n", "\n", "# TP1 :  Optimisation sans contraintes\n", "## M\u00e9thode des diff\u00e9rences finies pour $-u'' = f$\n", "\n", "### Existence et unicit\u00e9\n", "\n", "On veut approcher le probl\u00e8me $-u''=f$ avec conditions de Dirichlet : \n", "\n", "$$\n", "\\left\\{\\begin{aligned}\n", "&-u''(x) = f(x) \\hbox{ sur } (0,1) \\\\\n", "&u(0) = u(1) = 0\n", "\\end{aligned}\\right.$$\n", "\n", "par le syst\u00e8me lin\u00e9aire de dimension finie \n", "\n", "$$\n", "\\left\\{\\begin{aligned}\n", "&-\\frac{1}{h^2}(u_{i-1} - 2 u_i + u_{i+1}) = f_i \\hbox{ pour } 1\\leq i\\leq N\\\\\n", "&u_{0} = 0, u_{N+1} = 0 \n", "\\end{aligned}\\right.,$$\n", "\n", "o\u00f9 $h = 1/(n+1)$ et $x_j = h j$ pour $0\\leq j\\leq n$ et $f_j = f(t_j)$. \n", "\n", "**Q1)** Montrer que si on pose $ U = (u_1,\\dots, u_n)$ et $ F = (f_1,\\dots,f_n)$, le syst\u00e8me lin\u00e9aire peut \u00eatre \u00e9crit sous la forme $A_n  U = h^2 F$, o\u00f9 $A_n$ est une matrice sym\u00e9trique et d\u00e9finie positive. \n", "\n", "Le but de ce TP est donc de proposer et comparer des m\u00e9thodes de\n", "r\u00e9solution de syst\u00e8mes lin\u00e9aires du type $A_n x = b$, o\u00f9 $A$ est une\n", "matrice **sym\u00e9trique et d\u00e9finie positive**.\n", "En particulier on cherche \u00e0 r\u00e9soudre un syst\u00e8me lin\u00e9aire\n", " $A_n x = b$ par des m\u00e9thodes de descent de gradient en trouvant le minimiseur de \n", "\\begin{equation}\n", "\\label{problem}\n", "J(x) := \\frac{1}{2}\\sca{A_nx}{x} - \\sca{b}{x}.\n", "\\end{equation}\n", "\n", "**Q2)** On veut montrer existence et unicit\u00e9.\n", "\n", "**Q2.a)** Montrer que $J(x)\\to+\\infty$ quand $\\nr{x}\\to+\\infty$ et en d\u00e9duire l'existence d'un minimiseur.\n", "\n", "**Q2.b)** Calculer $\\nabla J(x)$ et $D^2J(x)$.\n", "\n", "**Q2.c)** Montrer que  tout minimiseur $x^\\star$ r\u00e9sout   l'\u00e9quation $A_n x = b$.\n", "\n", "**Q2.d)** Soit $x^\\star$ l'unique solution de $A_n x = b$, montrer que \n", "$$ J(x+x^\\star)=J(x^\\star)+\\frac{1}{2}\\sca{A_nx}{x} $$\n", "et en d\u00e9duire  que $x^\\star$ est l'unique minimiseur de $J$.\n", "\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Exercice 0\n", "\n", "**Q1)** Soit $f(x) = 1$, implementer en python deux fonctions `A(n)` et `F(x)`qui retournent la matrice $A_n$ et le vecteur $F$. "]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["**Q2)** Soit $n=30$, r\u00e9soudre le syst\u00e8me linaire en utilisant `np.linalg.solve`. Tracer sur la m\u00eame figure  la solution approch\u00e9e et la solution analytique."]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Exercice 1- La m\u00e9thode du gradient \u00e0 pas fixe\n", "\n", "Soit $\\mathcal J(x)$ la fonctionelle d\u00e9finie par \n", "\\begin{equation}\n", "\\mathcal J(x)=\\dfrac{1}{2}<x|A_n x>-<b|x>,\n", "\\end{equation}\n", "o\u00f9 $<\\cdot|\\cdot>$ d\u00e9signe le produit scalaire euclidien, $A_n\\in\\mathcal M_n(\\mathbb R)$ et $b\\in\\mathbb R^n$.\n", "\n", "**Q1)** Implementer $\\mathcal J $ et $\\nabla\\mathcal J$ en utilsant deux fonctions `J(x,A,b)` et  `gradJ(x,A,b)`.\n", "On rappelle que pour `A,b` donn\u00e9s, on peut d\u00e9finir une fonction dans la seule variable `x` comme `gradJn =lambda x : gradJ(x,A,b)`. Utiliser la fonction `verifier_gradient` pour varifier d'avoir bien calcul\u00e9 le gradient de $\\mathcal J$ en utilisant la matrice $A_n$ et le vecteur $F$ trouv\u00e9 dans l'exercice 0. \n", "\n"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["$$\\newcommand{\\nr}[1]{\\left\\Vert #1 \\right\\Vert}$$\n", "\n", "**Q2)** \n", "La m\u00e9thode de gradient \u00e0 pas constant $\\tau>0$ consiste \u00e0 construire une\n", "  suite minimisante d\u00e9finie par r\u00e9currence comme suit:\n", "    $$\\begin{cases} x_0 \\in \\mathbb R^n\\\\\n", "    d^{(k)} = -\\nabla J(x^{(k)}) \\\\\n", "   x^{(k+1)} = x^{(k)} + \\tau d^{(k)}\n", "  \\end{cases}$$\n", " Soit $T_\\tau(x)=x-\\tau\\nabla J(x)=(I_n-\\tau A)x+\\tau b$ tel que $x^{(k+1)}=T_\\tau(x^{(k)})$.\n", " \n", " **Q2.a)** Montrer que $x^\\star=T_{\\tau}(x^\\star)$ o\u00f9 $x^\\star$ est l'unique minimum de $J$.\n", " \n", " **Q2.b)** Donner une condition sur $\\tau$ telle que $T_\\tau$ est contactante.\n", " \n", " **Q2.c)** Montrer que la suite $(x^{(k)})$ converge vers $x^*$ quelque soit la valeur de $x_0$ et $\\tau$ v\u00e9rifiant la condition trouv\u00e9e pr\u00e9c\u00e9demment.\n", " \n", " **Q2.d)** *(bonus)* Montrer que le meilleur choix est $\\tau^\\star=\\dfrac{2}{\\lambda_{1}+\\lambda_{n}}$, o\u00f9 $\\lambda_1$ ($\\lambda_n$) est la plus petite (grande) valeur propre de $A_n$. Calculer $\\tau^\\star$ \u00e0 l'aide de la fonction `np.linalg.eigvalsh`.\n", " \n"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["**Q3)** Impl\u00e9menter l'algorithme du gradient \u00e0 travers une fonction de la forme \n", "\n", "`gradient_fixe(gradJ,x0,tau,epsilon,iterMax) `\n", "\n", "prennant en argument le gradient `gradJ`, la valeur initiale `x0`, le pas `tau`, le nombre maximal d'iterations autoris\u00e9es `IterMax` et la tolerance `epsilon`. Cette fonction devra retourner:\n", "- `x` dernier terme de la suite. \n", "- `err` liste de $\\parallel \\nabla \\mathcal J(\\bf u_k)\\parallel$.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["**Q4)**  Pour $n=20,50,100$ r\u00e9soudre le syst\u00e9me trouv\u00e9 dans l'exercice 0 et  tracer sur la m\u00eame figure les solutions approch\u00e9es. On prendra $\\tau=0.1$ et $\\epsilon=10^{-6}$. Tracer en \u00e9chelle logarithmique `err` en fonction du nombre des it\u00e9rations. Commenter les r\u00e9sultats.\n"]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["**Q5)** Reprendre l'exp\u00e9rience pr\u00e9c\u00e9dent en utlisant le meilleur choix de $\\tau^\\star$ trouv\u00e9 au d\u00e9but de l'exercice. Commenter les r\u00e9sultats. "]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Exercice 2- La m\u00e9thode du gradient \u00e0 pas optimal\n", "\n", "On rappelle que les iter\u00e9es de la  methode du gradient \u00e0 pas optimal sont d\u00e9finies comme\n", "\n", "$$\\begin{cases} d^{(k)} = - \\nabla \\mathcal J (x^{(k)}) \\\\\n", "    \\tau^{(k)} = \\frac{\\langle d^{(k)}|d^{(k)}\\rangle}{\\langle d^{(k)}|A d^{(k)}\\rangle}\\\\\n", "    x^{(k+1)} = x^{(k)} + \\tau^{(k)}d^{(k)}\n", "  \\end{cases}$$\n", "  \n", "**Q0)** On veut montrer que la suite des it\u00e9r\u00e9es $(x^{(k)})$ converge vers l'unique minimiseur $x^\\star$.\n", "\n", "**Q0.a)** Montrer que \n", "$$ J(x^{(k+1)})= J(x^{(k)})-\\dfrac{\\tau^{(k)}}{2}\\nr{\\nabla J(x^{(k)})}^2$$\n", "et en d\u00e9duire que\n", "$$ J(x^{(k+1)})\\leq J(x^{(k)})-\\dfrac{1}{2\\lambda_{n}}\\nr{\\nabla J(x^{(k)})}^2,$$\n", "o\u00f9 $\\lambda_n$ est la plus grande valeur propre de $A$. \n", "\n", "**Q0.b)** Montrer alors que pour tout $K\\geq 0$ on a l'in\u00e9galit\u00e9 suivante\n", "$$ \\sum_{0\\leq k\\leq K}\\nr{\\nabla J(x^{(k)})}^2\\leq 2\\lambda_n(J(x^{(0)})-J(x^\\star)) $$\n", "et en d\u00e9duire que $\\lim_{k\\to\\infty}\\nr{\\nabla J(x^{(k)})}=0$.\n", "\n", "**Q0.c)**  Conclure que \t$x^{(k)}\\to x^\\star$.\n", "\n", "  \n"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<div class=\"alert alert-block alert-danger\">\n", "<b>Vitesse de convergence </b>\n", "\n", "On d\u00e9finit le conditionnement de $A$ comme $\\kappa(A)=\\frac{\\lambda_n}{\\lambda_1}$ o\u00f9 $\\lambda_1$ ($\\lambda_n$) d\u00e9signe la plus petite (grande) valeur propre de $A$. Dans le cas du gradient \u00e0 pas optimal on obtient alors l'in\u00e9galit\u00e9 suivante \n", "$$ J(x^{(k)})-J(x^\\star)\\leq \\Big( \\dfrac{\\kappa(A)-1}{\\kappa(A)+1} \\Big)^{2k}(J(x^{(0)})-J(x^\\star)). $$\n", "On en d\u00e9duit que plus $\\kappa(A)$ est proche de $1$, plus la m\u00e9thode  du gradient \u00e0 pas optimal converge rapidement. \n", "Par contre lorsque $\\kappa(A)$ est grand, c'est-\u00e0-dire lorsque les valeurs propres extr\u00eames sont tr\u00e8s diff\u00e9rentes, la m\u00e9thode est tr\u00e8s lente.\n", "\n", "Pour plus des d\u00e9tails voir Hirriart-Urruty, *Optimisation et analyse convexe*, p 17-19 et p 53-56.\n", "</div>"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["**Q1)** Impl\u00e9menter l'algorithme du gradient \u00e0 pas optimal \u00e0 travers une fonction de la forme \n", "\n", "`gradient_optimal(J,gradJ,A,x0,epsilon,iterMax) `\n", "\n", "prennant en argument la fonction `J`, le gradient `gradJ`,la matrice `A`, la valeur initiale `x0`, le pas `tau`, le nombre maximal d'iterations autoris\u00e9es `IterMax` et la tolerance `epsilon`. Cette fonction devra retourner:\n", "- `x` dernier terme de la suite. \n", "- `err` liste de $\\parallel \\nabla \\mathcal J(\\bf u_k)\\parallel$.\n", "- `F` liste de $\\mathcal J(x^{(k)})$."]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["**Q2)**  Pour $n=20,50,100$ r\u00e9soudre le syst\u00e9me trouv\u00e9 dans l'exercice 0 et  tracer sur la m\u00eame figure les solutions approch\u00e9es. On prendra $\\epsilon=10^{-6}$. Tracer en \u00e9chelle logarithmique `err` en fonction du nombre des it\u00e9rations. Afficher pour chaque $n$ le conditionnement de $A_n$ et commenter les r\u00e9sultats."]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Exercice 3 - La m\u00e9thode du gradient conjugu\u00e9\n", "\n", "$\\newcommand{\\sca}[2]{\\langle#1|#2\\rangle}$\n", "\n", "L'id\u00e9e principale est de choisir des directions de descentes qui soient orthogonales les unes aux autres pour le produit scalaire $\\sca{x}{y}_A = \\sca{x}{A y}$ induit par la matrice $A$.  La m\u00e9thode\n", "  du gradient conjugu\u00e9 prend la forme suivante: on fixe $x_0=0$, puis\n", "  pour $k\\geq 0$,\n", "\\begin{equation}\n", "\\label{eq:a}\n", "\\left\\{\n", "\\begin{aligned}\n", "&g^{(k)} =  b - A x^{(k)}\\\\\n", "&p^{(k)} = g^{(k)}- \\frac{\\sca{Ap^{(k-1)}}{g^{(k)}}}{\\sca{A p^{(k-1)}}{p^{(k-1)}}} p^{(k-1)}\\\\\n", "&\\tau^{(k)} = \\frac{\\sca{p^{(k)}}{g^{(k)}}}{\\sca{A p^{(k)}}{p^{(k)}}}\\\\\n", "&x^{(k+1)} = x^{(k)} + \\tau^{(k)} p^{(k)}\n", "\\end{aligned}\n", "\\right.\n", "\\end{equation}\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["**Q1)** Impl\u00e9menter l'algorithme du gradient conjugu\u00e9 \u00e0 travers une fonction de la forme \n", "\n", "`gradient_conj(A,b,x0,epsilon,iterMax) `\n", "\n", "prennant en argument la matrice `A`, le vecteur `b`, la valeur initiale `x0`, le nombre maximal d'iterations autoris\u00e9es `IterMax` et la tolerance `epsilon`. Cette fonction devra retourner:\n", "- `x` dernier terme de la suite. \n", "- `err` liste de $\\parallel \\nabla \\mathcal J(\\bf u_k)\\parallel$.\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["**Q2)** Pour $n=20,50,100$ r\u00e9soudre le syst\u00e9me trouv\u00e9 dans l'exercice 0 et  tracer sur la m\u00eame figure les solutions approch\u00e9es. On prendra $\\epsilon=10^{-6}$. Tracer en \u00e9chelle logarithmique `err` en fonction du nombre des it\u00e9rations et commenter les r\u00e9sultats.\n"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Exercice 4- Application : r\u00e9gularisation de signaux \n", "\n", "Dans cette partie on s'int\u00e9resse \u00e0 un probl\u00e8me de d\u00e9bruitage. \u00c9tant donn\u00e9 un signal 1D $v:[0,1]\\to\\mathbb{R}$ continu et un param\u00e8tre $\\alpha > 0$, on cherche \u00e0 construire un signal r\u00e9gularis\u00e9 en r\u00e9solvant l'\u00e9quation diff\u00e9rentielle\n", "\n", "$$\n", "(D)\\hspace{3cm} \\left\\{\\begin{aligned}\n", "&-u''(t) + \\alpha (u(t) - v(t)) = 0 \\hbox{ sur } (0,1) \\\\\n", "&u'(0) = u'(1) = 0\n", "\\end{aligned}\\right.,$$\n", "\n", "que l'on discr\u00e9tise comme dans la section pr\u00e9c\u00e9dente."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["**Q1)** Poser $U = (u_0,\\dots, u_{N+1})$ et \u00e9crire le syst\u00e8me sous la forme $A_n U = F$ o\u00f9 la matrice $A_n$ et le vecteur $F$ sont \u00e0 construire. Montrer que la matrice $A_n$ est sym\u00e9trique et d\u00e9finie positive. \u00c9crire une fonction `Areg(n,alpha)` construisant la matrice $A_n$."]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "source": ["# <completer>\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": [" **Q2)**  En utilisant les agorithmes de descente de gradient, r\u00e9soudre le syst\u00e8me \u00e0 partir du signal $v$ donn\u00e9 ci-dessous (une somme de Gaussiennes auquelles on a rajout\u00e9 un bruit uniforme),  pour $\\alpha\\in\\{50,100,1000\\}$ et $n=50$. Tracer les solutions sur la m\u00eame figures et commenter."]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "source": ["n=100\n", "h=1/(n+1)\n", "t = np.linspace(0,1,n+2)\n", "v = 4*np.exp(-500*np.square(t-.8)) + np.exp(-50*np.square(t-.2))\n", "v = v + .5*np.random.random(n+2) \n", "# <completer>\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": [], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": [], "outputs": []}], "nbformat": 4}