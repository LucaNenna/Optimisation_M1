{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## **Le fichier *nom_prenom_OPT_M1_TP2*.ipynb est \u00e0 rendre avant samedi 21 d\u00e9cembre \u00e0 15h (heure de Seattle)**\n", "\n", "# TP2 : Optimisation sous contraintes\n", "\n", "## Patie I Le mod\u00e8le\n", "\n", "Dans la suite nous consid\u00e9rons $\\Omega=(0,1)$ et regarderons le probl\u00e8me de minimisation suivant :\n", "\\begin{equation}\n", "\\label{P}\n", "\\min_{u\\in\\mathcal{K}}\\mathcal{J}[u],\n", "\\tag{$\\mathcal{P}$}\n", "\\end{equation}\n", "o\u00f9 $$ \\mathcal{J}[u]:=\\int_\\Omega (\\frac{1}{2}u'(x)^2-f(x)u(x))dx $$\n", "et\n", "$$ \\mathcal K:=\\{ u\\in H^1(\\Omega)\\;|\\;u(0)=a,u(1)=b,\\; u(x)\\geq g(x)\\}, $$\n", "o\u00f9 $f$ et $g$ sont deux fonctions continues sur $\\Omega$.\n", "\n", "\n", "Dans ce TP on veut se focaliser sur  la m\u00e9thode du **gradient  projet\u00e9** et celle de la **p\u00e9nalisation ext\u00e9rieure**.\n", "\n", "Comme dans le TP 1 on discr\u00e9tise le probl\u00e8me par \u00e9lements finis. On consid\u00e8re un maillage uniforme de $\\Omega$ donn\u00e9 par $0=x_0<x_1<\\cdots<x_{N+1}=1$ avec $h=x_{i+1}-x_{i}$ et \n", "$$ \\mathcal K_h:=\\{ v_h\\in\\mathcal C^0(\\Omega)\\;|\\; v_h(x)=v_h(x_i)+(x-x_i)\\frac{v_h(x_{i+1})-v_h(x_{i})}{h_i}\\;\\forall x\\in[x_i,x_{i+1}]\\; v(x_i)\\geq g(x_i)\\;i=0,\\cdots,N+1\\;\\text{et}\\; v_h(0)=a,v_h(1)=b\\}.$$\n", "\n", "On approche \\eqref{P} par le probl\u00e8me suivant:\n", "\\begin{equation}\n", "\\label{Ph}\n", "\\min_{u_h\\in\\mathcal{K}_h}\\mathcal{J}[u_h],\n", "\\tag{$\\mathcal{P}_h$}\n", "\\end{equation}\n", "o\u00f9\n", "$$ \\mathcal{J}[u_h]:=\\int_\\Omega \\Big(\\frac{1}{2}u_h'(x)^2-f(x)u_h(x)\\Big)dx. $$\n", "\n", "**QI.1)** Comme dans le TP1 (**REPETITA IUVANT**) expliciter le probl\u00e8me \\eqref{Ph} en utilisant la formule de trap\u00e8zes pour approcher les int\u00e9grales et montrer qu'il peut se mettre sous la forme \n", "\n", "\\begin{equation}\n", "\\label{Pn}\n", "\\begin{cases}\n", "&\\min_{u_N\\in \\mathcal K_N}\\mathcal J_N[u_N] \\\\\n", "&\\mathcal K_N=\\{v_N\\in\\mathbb{R}^N\\;|\\; v_i\\geq g(x_i)\\;\\forall i \\}\n", "\\end{cases}\n", "\\tag{$\\mathcal P_N$}\n", "\\end{equation}\n", "o\u00f9 $\\mathcal J_N$ est \u00e0 expliciter. \n", "\n", "**QI.2)** Calculer le gradient $\\nabla \\mathcal J_N$ de $\\mathcal J_N$.\n", "\n", "**QI.3)** Montrer que \\eqref{Pn} admet une unique solution.\n", "\n", "**QI.4)** Impl\u00e9menter $\\mathcal J_N$ et $\\nabla \\mathcal J_N$ \u00e0 travers les deux fonction Python\n", "`J(U,a,b,fx)` et `DJ(U,a,b,fx)` (attention aux conditions au bord!!).\n", "\n", "**QI.5)** Pour $a=0$, $b=0$, $N=100$, $f(x)=1$, $g(x)=\\max(1.5-20(x-0.4)^2,0)$ :\n", "- r\u00e9soudre \\eqref{Pn} en utilisant `scipy.optimize.minimize`"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {}, "source": ["import numpy as np\n", "from scipy.optimize import minimize\n", "...\n", "''' Definir ou importer f, g, J et DJ '''\n", "...\n", "N = 100\n", "\n", "a, b, x = 0, 0, np.linspace(0,1, N + 2)\n", "\n", "fx, gx = f(x), g(x)\n", "\n", "Jf = lambda u : J(u, a, b, x, fx)\n", "\n", "DJf = lambda u : DJ(u, a, b, x, fx)\n", "\n", "cons = ({'type': 'ineq','fun' : lambda x: x - gx[1:-1], 'jac' : lambda x: np.eye(np.size(x))})\n", "\n", "u = np.zeros(n)\n", "\n", "res = minimize(Jf, u, method='SLSQP', jac=DJf, constraints=cons, tol=1e-8,\n", "\n", "               options={'xtol': 1e-8, 'disp': True,'maxiter':5000})\n", "# res.x contient la solution"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["- repr\u00e9senter sur le m\u00eame graphique la solution ainsi que l'obstacle. Tester aussi $f(x)=\\pi^2\\sin(\\pi x)$\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Parite II Le gradient projet\u00e9\n", "\n", "L'algorithme du gradient projet\u00e9 \u00e0 pas fixe pour minimiser une fonctionelle $\\mathcal J_N:\\mathbb{R}^N\\rightarrow\\mathbb{R}$ sur un esemble $\\mathcal K _N$ pour un point de d\u00e9part $\\bf u_0$ et un pas $t$  est donn\u00e9 par \n", "\n", "\\begin{equation}\n", "\\label{GrdPro}\n", "\\begin{cases}\n", "& \\bf d_k=-\\nabla \\mathcal J_N[\\bf u_k],\\\\\n", "& \\bf u_{k+1}= P_{\\mathcal K_N}(\\bf u_k+t \\bf d_k),\n", "\\end{cases}\n", "\\tag{$\\mathcal{GP}$}\n", "\\end{equation}\n", "\n", "o\u00f9 $P_{\\mathcal K_N}$ d\u00e9signe la projection sur $\\mathcal K_N$.\n", "On arr\u00eatera les it\u00e9rations lorsque $\\parallel \\bf u_{k+1}-\\bf u_k \\parallel \\leq \\epsilon$.\n", "\n", "<div class=\"alert alert-block alert-danger\">\n", "<b>Projection sur un convexe </b>\n", "Soit $K\\subseteq \\mathbb{R}^d$ un convexe ferm\u00e9 et $x\\in \\mathbb{R}^d$. Alors il y a equivalence entre \n", "- $p=P_K(x)$\n", "- $\\forall y\\in K$ $<p-x,p-y> \\leq 0$\n", "</div>\n", "\n", "**QII.1)**\n", "<b>Etude th\u00e9orique : convergece de la m\u00e9thode</b>\n", "Soit $\\mathcal J[\\bf u]$ de la forme $\\mathcal J[\\bf u]=\\frac{1}{2}<\\bf u, A \\bf u>-<\\bf b, \\bf u>$ o\u00f9 $A$ est une matrice sym\u00e9trique et d\u00e9finie positive. $\\mathcal K_N$ est un convexe ferm\u00e9 non vide. On admet que $x\\mapsto P_{\\mathcal K_N}(x)$ est 1-Lip par rapport \u00e0 la norme euclidienne. On veut montrer le th\u00e9or\u00e8me suivant :\n", "\n", "*Si $t\\in(0,\\dfrac{2}{\\lambda_{max}})$ (o\u00f9 $t$ est le pas de l'algo \\eqref{GrdPro} et $\\lambda_{max}$ est la plus grande valeur propre de la matrice $A$), alors quel que soit $\\bf u_{0}$, la suite $\\{\\bf u_k\\}_{k\\geq0}$ d\u00e9finie par le gradient projet\u00e9 converge vers le minimum $\\bf u^\\star$*\n", "\n", "(Indication : montrer d'abord que $\\parallel \\bf u_{k+1}-\\bf u^\\star\\parallel \\leq C(t)\\parallel \\bf u_{k}-\\bf u^\\star\\parallel$)\n", "\n", "\n", "**QII.2)** Montrer que la projection sur $\\mathcal K_N$ est donn\u00e9e par\n", "$$(P_{\\mathcal K_N}(\\bf u))_i=\\max(u_i,g_i)\\;\\forall i=1,\\cdots,N $$\n", "et \u00e9crire un fonction `projK(u,gx)` qui prend en argument deux vecteurs `u` et `gx` et qui renvoie un vecteur $P_{\\mathcal K_N}(\\bf u)$.\n", "\n", "**QII.3)** Implementer une fonction `grad_proj(J,DJ,gx,u0,t,epsilon,iterMax)` qui prend en argument  la fonctionnelle `J` \u00e0 minimiser, le gradient `DJ`, la contrainte `gx`, la valeur initiale `u0`, le pas `t`, le nombre maximal d'iterations autoris\u00e9es `IterMax`, la tolerance `epsilon`. Cette fonction devra retourner:\n", "- `u` dernier terme de la suite $\\bf u_k$ si `store=0` ou tous les termes si `store=1`;\n", "\n", "- `iter` nombre d'it\u00e9rations effectu\u00e9es.\n", "\n", "- `err` liste de $\\parallel \\bf u_{k+1}-\\bf u_k \\parallel$.\n", "\n", "**QII.4)** Tester la focntion `grad_proj` pour $a=0$, $b=0$, $f(x)=1$, $g(x)=\\max(1.5-20(x-0.4)^2,0)$, $N=2,10,20,50,100$, $\\epsilon=10^{-5}$. Commenter les r\u00e9sultats obtenus en prennant d'abord $t>\\dfrac{2}{\\lambda_{max}}$ et puis \n", "$$t=\\dfrac{1}{\\lambda_{min}+\\lambda_{max}}, $$\n", "$\\lambda_{min},\\lambda_{max}$ sont respectivement la plus petite et la plus grande valeur propre de la matrice $A$ associ\u00e9e \u00e0 la fonctionnelle $\\mathcal J_N$.\n", "Afficher \u00e0 l'aide de la fonction `frpintf` le nombre d'it\u00e9rations ainsi que le temps de calcul pour chaque $N$. Tracer sur une m\u00eame figure les solutions approch\u00e9es $\\bf u_N$, ainsi que le graphe de la fonction $g$.\n", "\n", "**QII.5)** V\u00e9rifier que la solution (en faisant varier $N$) satisfait\n", "\\begin{equation}\n", "\\begin{cases}\n", "&u(x)''\\leq f(x),\\\\\n", "&u(x)\\leq g(x),\\\\\n", "&(-u(x)''-f(x))(u(x)-g(x))=0,\n", "\\end{cases}\n", "\\end{equation}\n", "sur $\\Omega$. La premi\u00e8re \u00e9quation traduit une concavit\u00e9 maximale de la solution. La deuxi\u00e8me \u00e9quation repr\u00e9sente l'obstacle $g$. La troisi\u00e8me traduit le fait que l'on a au moins \u00e9galit\u00e9 dans une des deux \u00e9quations pr\u00e9c\u00e9dentes : soit on r\u00e9sout $-u(x)''=f(x)$, soit $u(x)=g(x)$ et $u$ est sur l'obstacle.\n", "\n", "**QII.6)** Reprendre la question pr\u00e9c\u00e9dente pour $f(x)=\\pi^2\\sin(\\pi x)$.\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Partie III M\u00e9thode de p\u00e9nalisation ext\u00e9rieure\n", "\n", "La m\u00e9thode p\u00e9nalisation ext\u00e9rieure consiste \u00e0 remplacer le probl\u00e8me sous contrainte \\eqref{Pn} par une suite de probl\u00e8mes sans contraite qui converge vers \\eqref{Pn}.\n", "\n", "Soit $\\gamma > 0$ le parem\u00e8tre de p\u00e9nalisation et $\\mathcal J_N^\\gamma$ la fonctionnelle d\u00e9finie comme \n", "\\begin{equation}\n", "\\mathcal J_N^\\gamma[\\bf u]=\\mathcal J_N[\\bf u]+\\dfrac{1}{\\gamma}\\sum_{i=1}^N(\\max(g_i-u_i,0))^2.\n", "\\end{equation}\n", "\n", "Le minimiseur de $\\mathcal J_N^\\gamma$ converge vers le minimiseur de $\\mathcal J_N$ quand $\\gamma\\rightarrow 0$.\n", "\n", "**QIII.1)** Calculer le gradient $\\nabla \\mathcal J_N^\\gamma$.\n", "\n", "**QIII.2)** Implementer une fonction `penalisation(J,DJ,gx,gamma,u0,t,epsilon,iterMax)` pour trouver le minimiseur de $\\mathcal J_N^\\gamma$ en utilisant l'algorithme du gradient \u00e0 pas fixe (`t` est le pas de descente).\n", "\n", "**QIII.3)** Tester cette fonction avec les valeurs num\u00e9riques de (8) et (10) en prenant : $N=20$, $t=0.1$ et $\\gamma\\in\\{10^5,10^4,10^3,10^2,10,1,0.1\\}$. Afficher \u00e0 l'aide de la fonction `fprintf` le nombre d'it\u00e9rations ainsi que le temps de calcul pour chaque $\\gamma$. Tracer sur une m\u00eame figure les solutions $\\bf u^\\gamma_N$ ainsi que la fonction $g$ et le graphe de la soltuion de $-u(x)''=1$. Commenter les r\u00e9sultats obtenus pour les diff\u00e9rentes valeurs de $\\gamma$ et d\u00e9duire la n\u00e9cessit\u00e9 d'une m\u00e9thode variant le param\u00e8tre de p\u00e9nalisation.\n", "\n", "**QIII.4)** On va maintenant mettre en place un algorithme permettant d'ajuster le param\u00e8tre de p\u00e9nalisation \u00e0 chaque it\u00e9ration :\n", "- trouver la solution $\\bf u_{k+1}$ approch\u00e9e de $\\mathcal J^{\\gamma_k}_N$,\n", "- mettre \u00e0 jour le param\u00e8tre $\\gamma_{k+1}=\\dfrac{\\gamma_k}{100}$.\n", "\n", "Programmer alors une nouvelle fonction `penalisation_var(J,DJ,gx,u0,t,epsilon,iterMax)`.\n", "\n", "**QIII.5)** Tester cette nouvelle fonction avec les valeurs num\u00e9riques de (8) et (10) en prenant  $N=20$ et $t=0.1$. Commenter les r\u00e9sultats."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {}, "source": []}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {}, "source": []}], "nbformat_minor": 2, "metadata": {"language_info": {"version": "3.5.5", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "celltoolbar": "None"}, "nbformat": 4}