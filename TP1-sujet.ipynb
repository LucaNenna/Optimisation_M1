{"cells": [{"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["**Le fichier *nom_prenom_OPT_M1*.ipynb est \u00e0 rendre avant samedi 7 d\u00e9cembre \u00e0 24h (heure de Paris)**\n", "\n", "\n", "# TP1 : Optimisation sans contraintes and beyond\n", "\n", "Dans la suite nous consid\u00e9rons $\\Omega=(0,1)$ et regarderons le probl\u00e8me de minimisation suivant :\n", "\\begin{equation}\n", "\\label{P}\n", "\\min_{u\\in\\mathcal{K}}\\mathcal{J}[u],\n", "\\tag{$\\mathcal{P}$}\n", "\\end{equation}\n", "o\u00f9 $$ \\mathcal{J}[u]:=\\int_\\Omega (\\frac{1}{2}u'(x)^2-f(x)u(x))dx $$\n", "et\n", "$$ \\mathcal K:=\\{ u\\in H^1(\\Omega)\\;|\\;u(0)=a,u(1)=b\\; \\text{donn\u00e9s}\\}. $$\n", "\n", "On rappelle que $H^1(\\Omega):=\\{u\\;|\\; u\\in L^2,\\;u'\\in L^2 \\}$ o\u00f9 $u'$ est la deriv\u00e9e au sens des distributions et $L^2$ d\u00e9signe l'espace de Lebesgue des fonctions de carr\u00e9 int\u00e9grable.\n", "\n", "Dans ce TP on veut se focaliser sur les m\u00e9thodes de **gradient** (pas fixe et pas optimal) et la m\u00e9thode du **gradient conjugu\u00e9**.\n", "\n", "\n", "**Good to know : \u00e9l\u00e9ments finis de Lagrange, cas $P_1$**\n", "\n", "Un cadre adpat\u00e9 pour approcher la solution de ce probl\u00e8me est offert par la m\u00e9thode des \u00e9l\u00e9ments finis de Lagrange :\n", "soit $0=x_0<x_1<\\cdots<x_{N+1}=1$ un maillage de $\\Omega$.D\u00e9finissons alors l\u2019espace $\\mathcal V_h$, sous-espace de $\\mathcal K$ de dimension finie, par\n", "$$ \\mathcal V_h:=\\{ v_h\\in\\mathcal C^0(\\Omega)\\;|\\; v_h(x)=v_h(x_i)+(x-x_i)\\frac{v_h(x_{i+1})-v_h(x_{i})}{h_i}\\;\\forall x\\in[x_i,x_{i+1}]\\;\\text{et}\\; v_h(0)=a,v_h(1)=b\\}.$$\n", "On approche  la solution $u$ du probl\u00e8me par $u_h\\in\\mathcal V_h$. En paticulier, comme $\\mathcal V_h=span\\{\\phi_1,\\cdots,\\phi_N\\}$ o\u00f9 $\\phi_i(x_j)=\\delta_{ij}$, on peut decomposer $u_h$ sous la forme $u_h(x)=\\sum_{i=1}^{N}u_i\\phi_i(x)$.\n", "\n", "Le probl\u00e8me approch\u00e9 devient alors\n", "\\begin{equation}\n", "\\label{Ph}\n", "\\min_{u_h\\in\\mathcal V_h} J[u_h]\n", "\\tag{$\\mathcal{P}_h$}\n", "\\end{equation}\n"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["### Partie I : \u00c9quation de Laplace\n", "\n", "**QI.1**) Expliciter le probl\u00e8me $(\\mathcal P_h)$ et montrer qu'il est \u00e9quivalent, si on approche les int\u00e9grale avec la formule de trap\u00e8zes, au suivant\n", "$$ \\min_{u_N\\in\\mathbb{R}^N} \\mathcal J_N[u_N]$$\n", "o\u00f9 \n", "\\begin{equation}\n", "\\label{PN}\n", "\\mathcal J_N[V]=\\sum_{i=0}^N\\Big(\\dfrac{(V_{i+1}-V_i)^2}{2h_i}-\\frac{h_i}{2}(f_{i+1}V_{i+1}+f_iV_i)\\Big),\n", "\\tag{$\\mathcal P_N$}\n", "\\end{equation}\n", "$f_i=f(x_i)$, $h_i=x_{i+1}-x_i$ et $V_{N+1}=b, V_0=a$.\n", "\n", "**QI.2**) Montrer  que la fonctionnelle peut \u00eatre \u00e9crite sous la forme $\\mathcal J_N[\\bf u]=\\frac{1}{2}<\\bf u, A\\bf u>-<\\bf b, \\bf u>$ o\u00f9 $A\\in\\mathcal{M}_N(\\mathbb{R})$ est une matrice sym\u00e9trique et d\u00e9finie positive et $<\\cdot,\\cdot>$ d\u00e9signe le produit scalaire euclidien.\n", "Montrer en suite que \\eqref{PN} peut se mettre sous la forme \n", "$$ A_N\\bf{u}_N=\\bf{f}_N $$\n", "o\u00f9  $\\bf{f}_N$ est \u00e0 preciser.\n", "\n", "**QI.3**) Y-a-t'il une relation entre la solution du probl\u00e8me \\eqref{PN} et celle du probl\u00e8me ci-dessous discretis\u00e9 pas les \u00e9l\u00e9ments finis P1 Lagrange sur le m\u00eame mailage avec la m\u00eame formule de quadrature?\n", "\\begin{equation}\n", "\\label{Lap}\n", "\\begin{cases}\n", "&-u''(x)=f(x),\\;x\\in\\Omega\\\\\n", "& u(0)=a,\\;u(1)=b.\n", "\\end{cases}\n", "\\tag{$\\mathcal L$}\n", "\\end{equation}\n", "\n", "**QI.4**) On consid\u00e8re $f(x)=-1$, $a=0$, $b=\\frac{1}{2}$ de sorte que la solution exacte de \\eqref{Lap} est $u(x)=\\frac{x^2}{2}$.\n", "R\u00e9soudre le syst\u00e8me lin\u00e9aire trouv\u00e9 au point (2) en utilisant `numpy.linalg.solve`. R\u00e9soudre directement \\eqref{PN} en utilisant `scipy.optimize.minimize` et comparer les deux solutions. \n", "\n", "**DANS LA SUITE ON PRENDRA $f=1$, $a=b=0$**\n", "\n", "**QI.5**) Implementer $\\mathcal J_N$ et $\\nabla\\mathcal J_N$ en utilsant les fonctions `J(U,a,b,x,f)` et `DJ(U,a,b,x,f)`.\n", "On rappelle que pour `a,b,x,f` donn\u00e9s, on peut d\u00e9finir une fonction dans la seule variable `V` comme `JN =lambda V : J(V,a,b,x,f)`.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# on importe les modules numpy et pyplot\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["### Partie II : M\u00e9thode du gradient \u00e0 pas fixe\n", "\n", "Soient  $\\bf u_0$ un point de d\u00e9part,  $t$ le pas de descente et  $\\epsilon$ une tolerance alors \n", " l'algorithme du gradient \u00e0 pas fixe pour une fonctionnelle $\\mathcal J_N :\\mathbb R^N\\rightarrow \\mathbb R$ s'\u00e9crit comme\n", "\\begin{equation}\n", "\\label{grad}\n", "\\begin{cases}\n", "&\\bf{d}_k= -\\nabla\\mathcal J_N(\\bf u_k),\\\\\n", "&\\bf u_{k+1}=\\bf u_k+t\\bf d_{k}.\n", "\\end{cases}\n", "\\end{equation}\n", "On arr\u00eatera les it\u00e9rations lorsque $\\parallel \\nabla \\mathcal J_N(\\bf u_k)\\parallel\\leq \\epsilon$.\n", "\n", "\n", "\n", "**QII.1**) **Etude th\u00e9orique : convergence lin\u00e9aire**\n", "- On rappelle que si $A\\in\\mathcal{M}_n(\\mathbb{R})$ est sym\u00e9trique et $\\lambda_1,\\cdots,\\lambda_n\\in\\sigma(A)$ alors on a $\\parallel Ax\\parallel\\leq \\max_i|\\lambda_i|\\parallel x\\parallel$ pour tout $x\\in\\mathbb{R}^n$. Montrer que pour $t\\in(0,\\dfrac{2}{\\lambda_{max}})$ on a l'inegalit\u00e9 suivante\n", "$$ \\parallel x_{k+1}-x_{k}\\parallel\\leq \\max(|1-t\\lambda_{min}|,|1-t\\lambda_{max}|)\\parallel x_{k}-x_{k-1}\\parallel, $$\n", "o\u00f9 $\\lambda_{min}$ (resp. $\\lambda_{max}$) est la plus petite (resp. grande) valeur propre de A.\n", "En deduire que pour rendre le taux de convergence le plus petit possible, le meilleur choix est $t=\\dfrac{2}{\\lambda_{min}+\\lambda_{max}}$. Calculer le taux et le commenter dans le cas o\u00f9 $\\lambda_{min}<<\\lambda_{max}$. \n", "\n", "**QII.2**) Impl\u00e9menter l'algorithme du gradient \u00e0 pas fixe \u00e0 travers une fonction de la forme \n", "\n", "`gradient_fixe(J,DJ,u0,t,epsilon,iterMax,store) `\n", "\n", "prenant en argument la fonctionnelle `J` \u00e0 minimiser, le gradient `DJ`, la valeur initiale `u0`, le pas `t`, le nombre maximal d'iterations autoris\u00e9es `IterMax`, la tolerance `epsilon` et un parametre `store` pilotant le type de stockage dans `u` (0 ou 1). Cette fonction devra retourner:\n", "- `u` dernier terme de la suite $\\bf u_k$ si `store=0` ou tous les termes si `store=1`;\n", "\n", "- `iter` nombre d'it\u00e9rations effectu\u00e9es.\n", "\n", "- `err` liste de $\\parallel \\nabla \\mathcal J_N(\\bf u_k)\\parallel$.\n", "\n", "\n", "**QII.3**) Tracer sur une m\u00eame figure les courbes de niveaux de $\\mathcal J_2$ (**N=2**) ainsi que le champ de vecteurs $\\nabla\\mathcal{J}_2$ sur $[-10,10]^2$. (Utiliser les fonctions `matplotlib.pyplot.contour` et `matplotlib.pyplot.quiver`).\n", "\n", "**QII.4**) Calculer les it\u00e9rations $\\bf u_k$ donn\u00e9es par l'algorithme et tracer sur la m\u00eame figure que pr\u00e9c\u00e9demment la ligne qui relie les $\\bf u_k$. On prendra $\\bf u_0=(8,4)$, $t=0.1$ et $\\epsilon=10^{-8}$. \n", "\n", "**QII.5**) Pour $N=2,5,20,50,100,200$. Afficher \u00e0 l'aide de la fonction `fprintf` le nombre d'it\u00e9rations ainsi que le temps de calcul pour chaque $N$. Tracer sur la m\u00eame figure les solutions approch\u00e9es ainsi que la solution exacte de \\eqref{Lap}. On prendra  $t=0.1$ et $\\epsilon=10^{-8}$. Tracer en \u00e9chelle logarithmique `err` en fonction du nombre des it\u00e9rations. Commenter les r\u00e9sultats.\n", "\n", "**QII.6**) Reprendre l'exp\u00e9rience pr\u00e9c\u00e9dent en utlisant $t\\in(0,\\dfrac{2}{\\lambda_{max}})$ et puis le meilleur choix de $t$ trouv\u00e9 au d\u00e9but de l'exercice. Commenter les r\u00e9sultats. "]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["### Partie III : M\u00e9thode du gradient \u00e0 pas variable\n", "\n", "\n", "La suite $\\bf u_k$  des it\u00e9r\u00e9es de l'algorithme de descente de gradient \u00e0 pas variable est donn\u00e9e par\n", "\n", "\\begin{equation}\n", "\\begin{cases}\n", "&\\bf{d}_k= -\\nabla\\mathcal J_N(\\bf u_k),\\\\\n", "&\\bf u_{k+1}=\\bf u_k+t_k\\bf d_{k},\n", "\\end{cases}\n", "\\end{equation}\n", "\n", "o\u00f9 $t_k$ minimise $ t\\mapsto \\mathcal J_N(\\bf u_k + t \\bf d_k) .$\n", "On arr\u00eatera les it\u00e9rations lorsque $\\parallel \\nabla \\mathcal J_N(\\bf u_k)\\parallel\\leq \\epsilon$.\n", "\n", "**QIII.1**) Calculer analytiquement le pas optimal $t_k$ pour $\\mathcal J_N$.\n", "\n", "\n", "**QIII.2**) Programmer une fonction `gradient_pasOptimal(J,DJ,u0,t0,epsilon,iterMax,store, optim1D)` o\u00f9 `optim1D` sera un entier identifiant la m\u00e9thode \u00e0 utiliser pour calculer le pas optimal. La fonction devra retourner `u` dernier terme de la suite $\\bf u_k$ si `store=0` ou tous les termes si `store=1`, `iter` nombre d'it\u00e9rations effectu\u00e9es, `tL` liste des pas optimaux et `err` liste de $\\parallel \\nabla \\mathcal J_N(\\bf u_k)\\parallel$.\n", "\n", "**QIII.3**) Cas $N=2$. Reprendre les exp\u00e9riences effectu\u00e9es dans la m\u00e9thode du gradient \u00e0 pas fixe.\n", "\n", "**QIII.4**) Reprendre la question (9). Pour chaque valeur de $N$, tracer un graphique en \u00e9chelle logarithmique de `err` obtenu en utilisant le pas optimal.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["### Partie IV : M\u00e9thode du gradient conjugu\u00e9\n", "\n", "On rappelle que la m\u00e9thode du gradient conjugu\u00e9 n'est valable que pour des fonctionnelles de la forme $\\mathcal J_N[\\bf u]=\\frac{1}{2}<\\bf u, A\\bf u>-<\\bf b, \\bf u>$ o\u00f9 $A\\in\\mathcal{M}_N(\\mathbb{R})$ est une matrice sym\u00e9trique et d\u00e9finie positive et $<\\cdot,\\cdot>$ d\u00e9signe le produit scalaire euclidien. L'algorithme est alors donn\u00e9 par\n", "\n", "\\begin{equation}\n", "\\begin{cases}\n", "&t_k=-\\dfrac{<\\bf g_k,\\bf d_k>}{<\\bf d_k,A\\bf d_k>},\\\\\n", "&\\bf u_{k+1}=\\bf u_k+t_k\\bf d_{k},\\\\\n", "&\\bf g_{k+1}=A \\bf u_{k+1}-\\bf b,\\\\\n", "&\\bf \\beta_{k}=\\dfrac{<\\bf g_{k+1},A\\bf d_{k}>}{<\\bf d_{k},A\\bf d_{k}>},\\\\\n", "&\\bf{d}_{k+1}= -\\bf g_{k+1}+\\beta_k\\bf d_k,\n", "\\end{cases}\n", "\\end{equation}\n", "\n", "**QIV.1**) Montrer que :\n", "- $<\\bf d_k,A\\bf d_i>=0$ $\\forall i<k$;\n", "- $t_k=\\dfrac{\\parallel\\bf g_k\\parallel^2}{<\\bf d_k,A\\bf d_k>}$;\n", "- $\\bf g_{k+1}=\\bf g_k+t_k A\\bf d_k$;\n", "- $<\\bf g_{k+1},\\bf g_{k}>=0$; \n", "- $\\beta_{k}=\\dfrac{\\parallel\\bf g_{k+1}\\parallel^2}{\\parallel\\bf g_{k}\\parallel^2}$\n", "\n", "**QIV.2**) Programmer la fonction `gradient_conjugue(J,DJ, u0, epsilon, iterMax, store)` qui retournera `u` dernier terme de la suite $\\bf u_k$ si `store=0` ou tous les termes si `store=1`, `iter` nombre d'it\u00e9rations effectu\u00e9es, `tL` liste des pas gener\u00e9s par l'agorithme et `err` liste de $\\parallel \\nabla \\mathcal J_N(\\bf u_k)\\parallel$. On utilisera comme test d'arr\u00eat $\\parallel \\bf g_k\\parallel <\\epsilon$.\n", "\n", "**QIV.3**) Cas $N=2$. Reprendre les exp\u00e9riences effectu\u00e9es dans la m\u00e9thode du gradient \u00e0 pas fixe.\n", "\n", "**QIV.4**) Reprendre la question (9).\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["### Partie V : M\u00e9thode de Newton\n", "\n", "On consid\u00e8re  maintenant la m\u00e9thode de Newton :\n", "\n", "\\begin{equation}\n", "\\label{N}\n", "\\begin{cases}\n", "&\\bf {d}_{k}= - \\nabla^2\\mathcal{J}_N(\\bf u_k)^{-1}\\nabla \\mathcal J_N(\\bf u_k),\\\\\n", "&\\bf u_{k+1}=\\bf u_k+\\bf d_{k}.\\\\\n", "\\end{cases}\n", "\\end{equation}\n", "\n", "\n", "**QV.1**) Si $\\bf u_0=\\bf 0$ apr\u00e8s combien d'it\u00e9rations la m\u00e9thode converge?\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.0"}, "celltoolbar": "None"}, "nbformat": 4, "nbformat_minor": 2}