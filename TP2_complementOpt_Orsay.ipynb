{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Le fichier *nom_prenom_OPT_M1_TP2*.ipynb + le fichier *nom_prenom_OPT_M1_TP1*.ipynb (du TP1 avec les parties que vous n'avez pas terminé) sont à rendre avant samedi 22 décembre à 15h (heure de Seattle)**\n",
    "\n",
    "# TP2 : Optimisation sous contraintes\n",
    "\n",
    "Dans la suite nous considérons $\\Omega=(0,1)$ et regarderons le problème de minimisation suivant :\n",
    "\\begin{equation}\n",
    "\\label{P}\n",
    "\\min_{u\\in\\mathcal{K}}\\mathcal{J}[u],\n",
    "\\tag{$\\mathcal{P}$}\n",
    "\\end{equation}\n",
    "où $$ \\mathcal{J}[u]:=\\int_\\Omega (\\frac{1}{2}u'(x)^2-f(x)u(x))dx $$\n",
    "et\n",
    "$$ \\mathcal K:=\\{ u\\in H^1(\\Omega)\\;|\\;u(0)=a,u(1)=b,\\; u(x)\\geq g(x)\\}, $$\n",
    "où $f$ et $g$ sont deux fonctions continues sur $\\Omega$.\n",
    "\n",
    "\n",
    "Dans ce TP on veut se focaliser sur  la méthode du **gradient de projeté** et celle de la **pénalisation extérieure**.\n",
    "\n",
    "Comme dans le TP 1 on discrétise le problème par élements finis. On considère un maillage uniforme de $\\Omega$ donné par $0=x_0<x_1<\\cdots<x_{N+1}=1$ avec $h=x_{i+1}-x_{i}$ et \n",
    "$$ \\mathcal K_h:=\\{ v_h\\in\\mathcal C^0(\\Omega)\\;|\\; v_h(x)=v_h(x_i)+(x-x_i)\\frac{v_h(x_{i+1})-v_h(x_{i})}{h_i}\\;\\forall x\\in[x_i,x_{i+1}]\\; v(x_i)\\geq g(x_i)\\;i=0,\\cdots,N+1\\;\\text{et}\\; v_h(0)=a,v_h(1)=b\\}.$$\n",
    "\n",
    "On approche \\eqref{P} par le problème suivant:\n",
    "\\begin{equation}\n",
    "\\label{Ph}\n",
    "\\min_{u_h\\in\\mathcal{K}_h}\\mathcal{J}[u_h],\n",
    "\\tag{$\\mathcal{P}_h$}\n",
    "\\end{equation}\n",
    "où\n",
    "$$ \\mathcal{J}[u_h]:=\\int_\\Omega \\Big(\\frac{1}{2}u_h'(x)^2-f(x)u_h(x)\\Big)dx. $$\n",
    "\n",
    "1- Comme dans le TP1 (**REPETITA IUVANT**) expliciter le problème \\eqref{Ph} en utilisant la formule de trapèzes pour approcher les integrales et montrer qu'il peut se mettre sous la forme \n",
    "\n",
    "\\begin{equation}\n",
    "\\label{Pn}\n",
    "\\begin{cases}\n",
    "&\\min_{u_N\\in \\mathcal K_N}\\mathcal J_N[u_N] \\\\\n",
    "&\\mathcal K_N=\\{v_N\\in\\mathbb{R}^N\\;|\\; v_i\\geq g(x_i)\\;\\forall i \\}\n",
    "\\end{cases}\n",
    "\\tag{$\\mathcal P_N$}\n",
    "\\end{equation}\n",
    "où $\\mathcal J_N$ est à expliciter. \n",
    "\n",
    "2- Calculer le gradient $\\nabla \\mathcal J_N$ de $\\mathcal J_N$.\n",
    "\n",
    "3- Montrer que \\eqref{P} admet une unique solution.\n",
    "\n",
    "4- Implémenter $\\mathcal J_N$ et $\\nabla \\mathcal J_N$ à travers les deux fonction Python\n",
    "`J(U,a,b,fx)` et `DJ(U,a,b,fx)` (attention aux conditions au bord!!).\n",
    "\n",
    "5- Pour $a=0$, $b=0$, $N=100$, $f(x)=1$, $g(x)=\\max(1.5-20(x-0.4)^2,0)$ :\n",
    "- résoudre \\eqref{Pn} en utilisant `scipy.optimize.minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "...\n",
    "''' Definir ou importer f, g, J et DJ '''\n",
    "...\n",
    "N = 100\n",
    "\n",
    "a, b, x = 0, 0, np.linspace(0,1, N + 2)\n",
    "\n",
    "fx, gx = f(x), g(x)\n",
    "\n",
    "Jf = lambda u : J(u, a, b, x, fx)\n",
    "\n",
    "DJf = lambda u : DJ(u, a, b, x, fx)\n",
    "\n",
    "cons = ({'type': 'ineq','fun' : lambda x: x - gx[1:-1], 'jac' : lambda x: np.eye(np.size(x))})\n",
    "\n",
    "u = np.zeros(n)\n",
    "\n",
    "res = minimize(Jf, u, method='SLSQP', jac=DJf, constraints=cons, tol=1e-8,\n",
    "\n",
    "               options={'xtol': 1e-8, 'disp': True,'maxiter':5000})\n",
    "# res.x contient la solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- représenter sur le même graphique la solution ainsi que l'obstacle. Tester aussi $f(x)=\\pi^2\\sin(\\pi x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient projeté\n",
    "\n",
    "L'algorithme du gradient projeté à pas fixe pour minimiser une fonctionelle $\\mathcal J_N:\\mathbb{R}^N\\rightarrow\\mathbb{R}$ sur un esemble $\\mathcal K _N$ pour un point de départ $\\bf u_0$ et un pas $\\alpha$  est donné par \n",
    "\n",
    "\\begin{equation}\n",
    "\\label{GrdPro}\n",
    "\\begin{cases}\n",
    "& \\bf d_k=-\\nabla \\mathcal J_N[\\bf u_k],\\\\\n",
    "& \\bf u_{k+1}= P_{\\mathcal K_N}(\\bf u_k+t \\bf d_k),\n",
    "\\end{cases}\n",
    "\\tag{$\\mathcal{GP}$}\n",
    "\\end{equation}\n",
    "\n",
    "où $P_{\\mathcal K_N}$ désigne la projection sur $\\mathcal K_N$\n",
    "On arrêtera les itérations lorsque $\\parallel \\bf u_{k+1}-\\bf u_k \\parallel \\leq \\epsilon$.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Etude théorique : convergece de la méthode</b>\n",
    "Soit $\\mathcal J[\\bf u]$ de la forme $\\mathcal J[\\bf u]=\\frac{1}{2}<\\bf u, A \\bf u>-<\\bf b, \\bf u>$ où $A$ est une matrice symétrique et définie positive. $\\mathcal K_N$ est un convexe fermé non vide. On admet que $x\\mapsto P_{\\mathcal K_N}(x)$ est 1-Lip par rapport à la norme euclidienne. On veut montrer le théorème suivant :\n",
    "\n",
    "*Si $t\\in(0,\\dfrac{2}{\\lambda_{max}})$ (où $t$ est le pas de l'algo \\eqref{GrdPro} et $\\lambda_{max}$ est la plus grande valeur propre de la matrice $A$), alors quel que soit $\\bf u_{0}$, la suite $\\{\\bf u_k\\}_{k\\geq0}$ définie par le gradient projeté converge vers le minimum $\\bf u^\\star$*\n",
    "\n",
    "(Indication : montrer d'abord que $\\parallel \\bf u_{k+1}-\\bf u^\\star\\parallel \\leq C(t)\\parallel \\bf u_{k}-\\bf u^\\star\\parallel$)\n",
    "</div>\n",
    "\n",
    "6- Montrer que la projection sur $\\mathcal K_N$ est donnée par\n",
    "$$(P_{\\mathcal K_N}(\\bf u))_i=\\max(u_i,g_i)\\;\\forall i=1,\\cdots,N $$\n",
    "et écrire un fonction `projK(u,gx)` qui prend en argument deux vecteurs `u` et `gx` et qui renvoie un vecteur $P_{\\mathcal K_N}(\\bf u)$.\n",
    "\n",
    "7- Implementer une fonction `grad_proj(J,DJ,gx,u0,t,epsilon,iterMax)` qui prend en argument  la fonctionnelle `J` à minimiser, le gradient `DJ`, la contrainte `gx`, la valeur initiale `u0`, le pas `t`, le nombre maximal d'iterations autorisées `IterMax`, la tolerance `epsilon`. Cette fonction devra retourner:\n",
    "- `u` dernier terme de la suite $\\bf u_k$ si `store=0` ou tous les termes si `store=1`;\n",
    "\n",
    "- `iter` nombre d'itérations effectuées.\n",
    "\n",
    "- `err` liste de $\\parallel \\bf u_{k+1}-\\bf u_k \\parallel$.\n",
    "\n",
    "8- Tester la focntion `grad_proj` pour $a=0$, $b=0$, $f(x)=1$, $g(x)=\\max(1.5-20(x-0.4)^2,0)$, $N=2,10,20,50,100$, $\\epsilon=10^{-5}$. Commenter les résultats obtenus en prennant d'abord $t>\\dfrac{2}{\\lambda_{max}}$ et puis \n",
    "$$t=\\dfrac{1}{\\lambda_{min}+\\lambda_{max}}, $$\n",
    "$\\lambda_{min},\\lambda_{max}$ sont respectivement la plus petite et la plus grande valeur propre de la matrice $A$ associée à la fonctionnelle $\\mathcal J_N$.\n",
    "Afficher à l'aide de la fonction `frpintf` le nombre d'itérations ainsi que le temps de calcul pour chaque $N$. Tracer sur une même figure les solutions approchées $\\bf u_N$, ainsi que le graphe de la fonction $g$.\n",
    "\n",
    "9- Vérifier que la solution (en faisant varier $N$) satisfait\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "&u(x)''\\leq f(x),\\\\\n",
    "&u(x)\\leq g(x),\\\\\n",
    "&(-u(x)''-f(x))(u(x)-g(x))=0,\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "sur $\\Omega$. La première équation traduit une concavité maximale de la solution. La deuxième équation représente l'obstacle $g$. La troisième traduit le fait que l'on a au moins égalité dans une des deux équations précédentes : soit on résout $-u(x)''=f(x)$, soit $u(x)=g(x)$ et $u$ est sur l'obstacle.\n",
    "\n",
    "10- Reprendre la question précédente pour $f(x)=\\pi^2\\sin(\\pi x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de pénalisation extérieure\n",
    "\n",
    "La méthode pénalisation extérieure consiste à remplacer le probléme sous contrainte \\eqref{Pn} par une suite de problèmes sans contraite qui converge vers \\eqref{Pn}.\n",
    "\n",
    "Soit $\\gamma > 0$ le paremètre de pénalisation et $\\mathcal J_N^\\gamma$ la fonctionnelle définie comme \n",
    "\\begin{equation}\n",
    "\\mathcal J_N^\\gamma[\\bf u]=\\mathcal J_N[\\bf u]+\\dfrac{1}{\\gamma}\\sum_{i=1}^N(\\max(u_i-g_i,0))^2.\n",
    "\\end{equation}\n",
    "\n",
    "Le minimiseur de $\\mathcal J_N^\\gamma$ converge vers le minimiseur de $\\mathcal J_N$ quand $\\gamma\\rightarrow 0$.\n",
    "\n",
    "11- Calculer le gradient $\\nabla \\mathcal J_N^\\gamma$.\n",
    "\n",
    "12- Implementer une fonction `penalisation(J,DJ,gx,gamma,u0,t,epsilon,iterMax)` pour trouver le minimiseur de $\\mathcal J_N^\\gamma$ en utilisant l'algorithme du gradient à pas fixe (`t` est le pas de descente).\n",
    "\n",
    "13- Tester cette fonction avec les valeurs numériques de (8) et (10) en prenant : $N=20$, $t=0.1$ et $\\gamma\\in\\{10^5,10^4,10^3,10^2,10,1,0.1\\}$. Afficher à l'aide de la fonction `fprintf` le nombre d'itérations ainsi que le temps de calcul pour chaque $\\gamma$. Tracer sur une même figure les solutions $\\bf u^\\gamma_N$ ainsi que la fonction $g$ et le graphe de la soltuion de $-u(x)''=1$. Commenter les résultats obtenus pour les différentes valeurs de $\\gamma$ et déduire la nécessité d'une méthode variant le paramètre de pénalisation.\n",
    "\n",
    "14- On va maintenant mettre en place un algorithme permettant d'ajuster le paramètre de pénalisation à chaque itération :\n",
    "- trouver la solution $\\bf u_{k+1}$ approchée de $\\mathcal J^{\\gamma_k}_N$,\n",
    "- mettre à jour le paramètre $\\gamma_{k+1}=\\dfrac{\\gamma_k}{100}$.\n",
    "\n",
    "Programmer alors une nouvelle fonction `penalisation_var(J,DJ,gx,u0,t,epsilon,iterMax)`.\n",
    "\n",
    "15- Tester cette nouvelle fonction avec les valeurs numériques de (8) et (10) en prenant  $N=20$ et $t=0.1$. Commenter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
